{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Solvers, Optimizers, and Automatic Differentiation](\"https://julia.quantecon.org/more_julia/optimization_solver_packages.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mActivated\u001b[0m /Users/taisei/Project.toml\u001b[39m\n",
      "\u001b[36m\u001b[1mInfo\u001b[0m Project name is quantecon-notebooks-julia, version is 0.8.0\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "using InstantiateFromURL\n",
    "# optionally add arguments to force installation: instantiate = true, precompile = true\n",
    "github_project(\"QuantEcon/quantecon-notebooks-julia\", version = \"0.8.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra, Statistics\n",
    "using ForwardDiff, Optim, JuMP, Ipopt, BlackBoxOptim, Roots, NLsolve, LeastSquaresOptim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Updating\u001b[22m\u001b[39m `~/Project.toml`\n",
      " \u001b[90m [e88e6eb3]\u001b[39m\u001b[91m - Zygote v0.3.2\u001b[39m\n",
      "\u001b[32m\u001b[1m   Updating\u001b[22m\u001b[39m `~/Manifest.toml`\n",
      " \u001b[90m [7869d1d1]\u001b[39m\u001b[91m - IRTools v0.2.3\u001b[39m\n",
      " \u001b[90m [e88e6eb3]\u001b[39m\u001b[91m - Zygote v0.3.2\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "#===using Pkg\n",
    "Pkg.rm(\"Zygote\")\n",
    "==#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling Zygote [e88e6eb3-aa80-5325-afca-941959d7151f]\n",
      "└ @ Base loading.jl:1260\n"
     ]
    }
   ],
   "source": [
    "#using Zygote\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Differentiable Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three ways to calculate the gradient or Jacobian\n",
    "\n",
    "1. Analytic derivatives/Symbolic differentiation\n",
    "\n",
    "2. Finite differences\n",
    "Tradeoff: Large $\\Delta$ is numerically stable but inaccurate.\n",
    "Use `DiffEqDiffTools.jl` to choose $\\Delta$\n",
    "\n",
    "3. Automatic Differentiation\n",
    "The same as analytic/symbolic differentiation, but where the **chain rule** is calculated **numerically**.\n",
    "e.g. d(sin(x))=>cos(x)dx\n",
    "\n",
    "Two basic approaces of AD\n",
    "\n",
    "1. reverse-mode : when a function is R^n to R\n",
    "\n",
    "2. forward-mode: when a function is R to R^N\n",
    "\n",
    "* chain rule\n",
    "\n",
    "$\\frac{dy}{dx}=\\frac{dy}{dw}\\frac{dw}{dx}$\n",
    "\n",
    "Forward-mode starts the calculation from the left with $\\frac{dy}{dw}$ then  calculates the product with $\\frac{dw}{dx}$.\n",
    "\n",
    "Reverse mode starts on the right hand side with $\\frac{dw}{dx}$ and works backwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x_1,x_2)=x_1x_2+\\sin(x_1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function f(x_1, x_2)\n",
    "    w_1 = x_1\n",
    "    w_2 = x_2\n",
    "    w_3 = w_1 * w_2\n",
    "    w_4 = sin(w_1)\n",
    "    w_5 = w_3 + w_4\n",
    "    return w_5\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The derivative of w_1 with respect to x_1 and w_2 with respect to x_2 are dual numbers.\n",
    "So if we have the function $f(x_1,x_2)$ and want to find the derivative of f(3.8,6.9), we would see them with the dual numbers x_1=(3.8, 1) and x_2= (6.9,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward-Mode Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First fix the variable you are interested in (called **seeding**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ForwardDiff.gradient(h, x) = [26.354764961030977 16.663053156992284]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5-element Array{Float64,1}:\n",
       " 0.9172415083404515\n",
       " 1.3255149730878326\n",
       " 1.1401222603500552\n",
       " 1.156746730742981\n",
       " 1.0846333355946316"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ForwardDiff\n",
    "h(x) = sin(x[1]) + x[1] * x[2] + sinh(x[1] * x[2]) # multivariate.\n",
    "x = [1.4 2.2]\n",
    "@show ForwardDiff.gradient(h,x) # use AD, seeds from x\n",
    "\n",
    "#Or, can use complicated functions of many variables\n",
    "f(x) = sum(sin, x) + prod(tan, x) * sum(sqrt, x)\n",
    "g = (x) -> ForwardDiff.gradient(f, x); # g() is now the gradient\n",
    "g(rand(5)) # gradient at a random point\n",
    "# ForwardDiff.hessian(f,x') # or the hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4142135623730951"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function squareroot(x) #pretending we don't know sqrt()\n",
    "    z = copy(x) # Initial starting point for Newton’s method\n",
    "    while abs(z*z - x) > 1e-13\n",
    "        z = z - (z*z-x)/(2z)\n",
    "    end\n",
    "    return z\n",
    "end\n",
    "squareroot(2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse-mode : `Zygote.jl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25.0, 2.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Zygote\n",
    "\n",
    "h(x, y) = 3x^2 + 2x + 1 + y*x - y\n",
    "gradient(h, 3.0, 5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum = 5.774181450804439 with in 2 iterations\n"
     ]
    }
   ],
   "source": [
    "using Optim, LinearAlgebra\n",
    "N = 1000000\n",
    "y = rand(N)\n",
    "λ = 0.01\n",
    "obj(x) = sum((x .- y).^2) + λ*norm(x)\n",
    "\n",
    "x_iv = rand(N)\n",
    "function g!(G, x)\n",
    "    G .=  obj'(x)\n",
    "end\n",
    "\n",
    "results = optimize(obj, g!, x_iv, LBFGS()) # or ConjugateGradient()\n",
    "println(\"minimum = $(results.minimum) with in \"*\n",
    "\"$(results.iterations) iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.6536436208636119"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D(f) = x-> gradient(f, x)[1]  # returns first in tuple\n",
    "\n",
    "D_sin = D(sin)\n",
    "D_sin(4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Functions on Bounded Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Results of Optimization Algorithm\n",
       " * Algorithm: Brent's Method\n",
       " * Search Interval: [-2.000000, 1.000000]\n",
       " * Minimizer: 0.000000e+00\n",
       " * Minimum: 0.000000e+00\n",
       " * Iterations: 5\n",
       " * Convergence: max(|x - x_upper|, |x - x_lower|) <= 2*(1.5e-08*|x|+2.2e-16): true\n",
       " * Objective Function Calls: 6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Optim\n",
    "using Optim: converged, maximum, minimizer, iterations\n",
    "\n",
    "result = optimize(x->x^2,-2.0,1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converged(result)||error(\"Failed to converge in $(iterations(result)) iterations\")\n",
    "xmin = result.minimizer\n",
    "result.minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Optim: maximizer\n",
    "f(x) = -x^2\n",
    "result = maximize(f, -2.0, 1.0)\n",
    "converged(result) || error(\"Failed to converge in $(iterations(result)) iterations\")\n",
    "xmin = maximizer(result)\n",
    "fmax = maximum(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unconstrained Multivariate Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " * Status: success\n",
       "\n",
       " * Candidate solution\n",
       "    Minimizer: [1.00e+00, 1.00e+00]\n",
       "    Minimum:   3.525527e-09\n",
       "\n",
       " * Found with\n",
       "    Algorithm:     Nelder-Mead\n",
       "    Initial Point: [0.00e+00, 0.00e+00]\n",
       "\n",
       " * Convergence measures\n",
       "    √(Σ(yᵢ-ȳ)²)/n ≤ 1.0e-08\n",
       "\n",
       " * Work counters\n",
       "    Seconds run:   0  (vs limit Inf)\n",
       "    Iterations:    60\n",
       "    f(x) calls:    118\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x) = (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\n",
    "x_iv = [0.0, 0.0]\n",
    "results = optimize(f, x_iv) # i.e. optimize(f, x_iv, NelderMead())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* L-BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum = 5.378388330692143e-17 with argmin = [0.9999999926662504, 0.9999999853325008] in 24 iterations\n"
     ]
    }
   ],
   "source": [
    "results = optimize(f, x_iv, LBFGS())\n",
    "println(\"minimum = $(results.minimum) with argmin = $(results.minimizer) in \"*\n",
    "\"$(results.iterations) iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As no derivative was given, it used finite differences to approximate the gradient of f(x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum = 5.191703158437428e-27 with argmin = [0.999999999999928, 0.9999999999998559] in 24 iterations\n"
     ]
    }
   ],
   "source": [
    "f(x) = (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\n",
    "x_iv = [0.0, 0.0]\n",
    "results = optimize(f, x_iv, LBFGS(), autodiff=:forward) # i.e. use ForwardDiff.jl\n",
    "println(\"minimum = $(results.minimum) with argmin = $(results.minimizer) in \"*\n",
    "\"$(results.iterations) iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we did not need to use `ForwardDiff.jl` directly, as long as our f(x) function was written to be generic (see the generic programming lecture )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analytical gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum = 5.191703158437428e-27 with argmin = [0.999999999999928, 0.9999999999998559] in 24 iterations\n"
     ]
    }
   ],
   "source": [
    "f(x) = (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\n",
    "x_iv = [0.0, 0.0]\n",
    "function g!(G, x)\n",
    "    G[1] = -2.0 * (1.0 - x[1]) - 400.0 * (x[2] - x[1]^2) * x[1]\n",
    "    G[2] = 200.0 * (x[2] - x[1]^2)\n",
    "end\n",
    "\n",
    "results = optimize(f, g!, x_iv, LBFGS()) # or ConjugateGradient()\n",
    "println(\"minimum = $(results.minimum) with argmin = $(results.minimizer) in \"*\n",
    "\"$(results.iterations) iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " * Status: failure (reached maximum number of iterations) (line search failed)\n",
       "\n",
       " * Candidate solution\n",
       "    Minimizer: [1.13e+00, 1.26e+00]\n",
       "    Minimum:   4.771724e-02\n",
       "\n",
       " * Found with\n",
       "    Algorithm:     Simulated Annealing\n",
       "    Initial Point: [0.00e+00, 0.00e+00]\n",
       "\n",
       " * Convergence measures\n",
       "    |x - x'|               = NaN ≰ 0.0e+00\n",
       "    |x - x'|/|x'|          = NaN ≰ 0.0e+00\n",
       "    |f(x) - f(x')|         = NaN ≰ 0.0e+00\n",
       "    |f(x) - f(x')|/|f(x')| = NaN ≰ 0.0e+00\n",
       "    |g(x)|                 = NaN ≰ 1.0e-08\n",
       "\n",
       " * Work counters\n",
       "    Seconds run:   0  (vs limit Inf)\n",
       "    Iterations:    1000\n",
       "    f(x) calls:    1001\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(x) = (1.0 - x[1])^2 + 100.0 * (x[2] - x[1]^2)^2\n",
    "x_iv = [0.0, 0.0]\n",
    "results = optimize(f, x_iv, SimulatedAnnealing()) # or ParticleSwarm() or NelderMead()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Systemps of Equations and Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeastSquaresOptim.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
